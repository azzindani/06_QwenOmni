{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1 - Works but need improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import os\n",
    "from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, BitsAndBytesConfig\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-Omni-3B'\n",
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map=\"cuda\",\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    ").to(device) #''\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Conversation history\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# Chat logic with audio input/output\n",
    "def voice_chat(audio_path, temperature, max_tokens):\n",
    "    # Add user audio to conversation\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"audio\", \"audio\": audio_path}]\n",
    "    })\n",
    "\n",
    "    # Prepare model input\n",
    "    text = processor.apply_chat_template(history, add_generation_prompt=True, tokenize=False)\n",
    "    audios, images, videos = process_mm_info(history, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=audios,\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "    ).to(model.device).to(model.dtype)\n",
    "\n",
    "    # Generate response (text + audio)\n",
    "    text_ids, audio = model.generate(\n",
    "        **inputs,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    text_response = processor.batch_decode(text_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Add model's response to conversation history\n",
    "    history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": text_response}]\n",
    "    })\n",
    "\n",
    "    # Save audio to output\n",
    "    output_path = \"output.wav\"\n",
    "    sf.write(output_path, audio.reshape(-1).detach().cpu().numpy(), samplerate=24000)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Build Gradio UI (audio-only)\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üéôÔ∏è Qwen2.5 Omni Voice Chat (Audio In/Out Only)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"üé§ Speak\", type=\"filepath\")\n",
    "        audio_output = gr.Audio(label=\"üîä Response\", type=\"filepath\", interactive=False)\n",
    "\n",
    "    with gr.Accordion(\"‚öôÔ∏è Parameters\", open=False):\n",
    "        temperature = gr.Slider(0, 1, step=0.1, value=0.6, label=\"Temperature\")\n",
    "        max_tokens = gr.Slider(128, 4096, step=1, value=256, label=\"Max new tokens\")\n",
    "\n",
    "    submit_btn = gr.Button(\"Send\")\n",
    "\n",
    "    # Event handler\n",
    "    submit_btn.click(\n",
    "        fn=voice_chat,\n",
    "        inputs=[audio_input, temperature, max_tokens],\n",
    "        outputs=audio_output\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2 - works and fail in direct generation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import os\n",
    "from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, BitsAndBytesConfig\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = \"Qwen/Qwen2.5-Omni-3B\"\n",
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map=\"cuda\",\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    ").to(device) #''\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Conversation history\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# Chat logic with audio input/output\n",
    "def voice_chat(audio_path, temperature, max_tokens):\n",
    "    # Add user audio to conversation\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"audio\", \"audio\": audio_path}]\n",
    "    })\n",
    "\n",
    "    # Prepare model input\n",
    "    text = processor.apply_chat_template(history, add_generation_prompt=True, tokenize=False)\n",
    "    audios, images, videos = process_mm_info(history, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=audios,\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "    ).to(model.device).to(model.dtype)\n",
    "\n",
    "    # Generate response (text + audio)\n",
    "    text_ids, audio = model.generate(\n",
    "        **inputs,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    text_response = processor.batch_decode(text_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Add model's response to conversation history\n",
    "    history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": text_response}]\n",
    "    })\n",
    "\n",
    "    # Save audio to output\n",
    "    output_path = \"output.wav\"\n",
    "    sf.write(output_path, audio.reshape(-1).detach().cpu().numpy(), samplerate=24000)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Build Gradio UI (audio-only)\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üéôÔ∏è Qwen2.5 Omni Voice Chat (Audio In/Out Only)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"üé§ Speak\", type=\"filepath\")\n",
    "        audio_output = gr.Audio(label=\"üîä Response\", type=\"filepath\", interactive=True, autoplay=True)\n",
    "\n",
    "    with gr.Accordion(\"‚öôÔ∏è Parameters\", open=False):\n",
    "        temperature = gr.Slider(0, 1, step=0.1, value=0.6, label=\"Temperature\")\n",
    "        max_tokens = gr.Slider(128, 4096, step=1, value=256, label=\"Max new tokens\")\n",
    "\n",
    "    submit_btn = gr.Button(\"Send\")\n",
    "\n",
    "    # Event handler\n",
    "    submit_btn.click(\n",
    "        fn=voice_chat,\n",
    "        inputs=[audio_input, temperature, max_tokens],\n",
    "        outputs=audio_output\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-09T06:05:20.405Z",
     "iopub.execute_input": "2025-05-09T06:03:35.016315Z",
     "iopub.status.busy": "2025-05-09T06:03:35.014854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import os\n",
    "from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, BitsAndBytesConfig\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = \"Qwen/Qwen2.5-Omni-3B\"\n",
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map=\"cuda\",\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    ").to(device) #''\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Conversation history\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# Chat logic with audio input/output\n",
    "def voice_chat(audio_path, temperature, max_tokens):\n",
    "    # Add user audio to conversation\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"audio\", \"audio\": audio_path}]\n",
    "    })\n",
    "\n",
    "    # Prepare model input\n",
    "    text = processor.apply_chat_template(history, add_generation_prompt=True, tokenize=False)\n",
    "    audios, images, videos = process_mm_info(history, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=audios,\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "    ).to(model.device).to(model.dtype)\n",
    "\n",
    "    # Generate response (text + audio)\n",
    "    text_ids, audio = model.generate(\n",
    "        **inputs,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    text_response = processor.batch_decode(text_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Add model's response to conversation history\n",
    "    history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": text_response}]\n",
    "    })\n",
    "\n",
    "    # Save audio to output\n",
    "    output_path = \"output.wav\"\n",
    "    sf.write(output_path, audio.reshape(-1).detach().cpu().numpy(), samplerate=24000)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Build Gradio UI (audio-only)\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üéôÔ∏è Qwen2.5 Omni Voice Chat (Audio In/Out Only)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"üé§ Speak\", type=\"filepath\")\n",
    "        audio_output = gr.Audio(label=\"üîä Response\", type=\"filepath\", interactive=True, autoplay=True)\n",
    "\n",
    "    with gr.Accordion(\"‚öôÔ∏è Parameters\", open=False):\n",
    "        temperature = gr.Slider(0, 1, step=0.1, value=0.6, label=\"Temperature\")\n",
    "        max_tokens = gr.Slider(128, 4096, step=1, value=256, label=\"Max new tokens\")\n",
    "\n",
    "    submit_btn = gr.Button(\"Send\")\n",
    "\n",
    "    # Event handler\n",
    "    submit_btn.click(\n",
    "        fn=voice_chat,\n",
    "        inputs=[audio_input, temperature, max_tokens],\n",
    "        outputs=audio_output\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "omni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
